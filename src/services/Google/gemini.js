// node --version # Should be >= 18
// npm install @google/generative-ai
const {
    GoogleGenerativeAI,
    HarmCategory,
    HarmBlockThreshold,
  } = require("@google/generative-ai");

  async function runChat(configuration,api_key,service) {

    try {
        console.log("running gemini",configuration,api_key);
        const genAI = new GoogleGenerativeAI(api_key);
        const model = genAI.getGenerativeModel({ model: configuration?.model });
        console.log(model);
        delete configuration?.generationConfig?.model;
        configuration.safetySettings = [
            {
              category: HarmCategory.HARM_CATEGORY_HARASSMENT,
              threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
              threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
              threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
              threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
          ];
          console.log(configuration);
          switch (service) {
            case "chat":
              const chat = model.startChat({generationConfig: configuration?.generationConfig,history: configuration?.history, safetySettings: configuration?.safetySettings});
              const result = await chat.sendMessage(configuration?.user_input);
              const response = result.response;
              console.log(response.text());
              const history = await chat.getHistory(); 
              const msgContent = { role: "user", parts: [{ text: configuration?.user_input }] }; 
              const contents = [...history, msgContent];
              const { totalTokens:input_tokens } = await model.countTokens({ contents });
              const { totalTokens:output_tokens } = await model.countTokens({ contents:response.candidates[0].content });
              console.log(response.text())
              return {success:true,modelResponse:response,usage:{input_tokens:input_tokens,output_tokens:output_tokens,total_tokens:input_tokens+output_tokens},history:history}
            case "completion":
              const promptResponse=await model.generateContent(configuration?.prompt);
              const { totalTokens:prompt_tokens } = await model.countTokens(configuration?.prompt);
              const { totalTokens:completion_tokens } = await model.countTokens(promptResponse.response.text());
              return {success:true,modelResponse:promptResponse.response,usage:{input_tokens:prompt_tokens,output_tokens:completion_tokens,total_tokens:prompt_tokens+completion_tokens}}

            case "embedding":
              const embeddingResponse = await model.embedContent(configuration?.input || "");
              // const { totalTokens:embedding_tokens } = await model.countTokens(configuration?.input || "");
              // const { totalTokens:embedCompletion_tokens } = await model.countTokens(embeddingResponse.embedding.values);
              return {success:true,modelResponse:embeddingResponse.embedding,usage:{input_tokens:0,output_tokens:0,total_tokens:0}}
            
          }
          return {success:false,error:"operation undefined!"}
    } catch (error) {
        console.log("gemini error=>", error);
        return {success:false,error:error.message}
    }
}
module.exports={runChat}
   
  
//     const generationConfig = {
//       temperature: 0.9,
//       topK: 1,
//       topP: 1,
//       maxOutputTokens: 2048,
//     };
  
    
  
//     const chat = model.startChat({
//       generationConfig,
//       safetySettings,
//       history: [
//         {
//           role: "user",
//           parts: [{ text: "hello"}],
//         },
//         {
//           role: "model",
//           parts: [{ text: "Hello there! How can I assist you today?"}],
//         },
//         {
//           role: "user",
//           parts: [{ text: "How are u"}],
//         },
//         {
//           role: "model",
//           parts: [{ text: "I am well, thank you for asking. I am a large language model, designed to provide information and assist with a wide variety of inquiries and tasks. How are you doing today?"}],
//         },
//       ],
//     });
  
//     const result = await chat.sendMessage("YOUR_USER_INPUT");
//     const response = result.response;
//     console.log(response.text());
//   }
  
//   runChat();


